{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Consolidated Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr5xPNuHWXo9"
      },
      "source": [
        "Notebook for consolidated results. Path might need to be changed. The parquet file I have for 2006-2017 data is too large to upload to github, so I include a google drive link for it here. It is just the original data file downloaded from web, preprocessing is done with function in this notebook, so if you already have the csv file you can just change the code about read parquet to read csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxSrho25XSOZ"
      },
      "source": [
        "https://drive.google.com/file/d/1YOUE6DZBP445NEfzbOi907emhREZKUQr/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JhuEbDvDXZL",
        "outputId": "0ee1f3ac-e9e8-4078-e16f-978a1aeccf43"
      },
      "source": [
        "from google.colab import drive\n",
        "import os \n",
        "drive.mount('/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4FfGutDD4Fr",
        "outputId": "081362de-4d7e-4ed7-a2f8-2f2d309276a8"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5SMv1A-OZKO",
        "outputId": "1247c524-f833-47e8-e894-c37e0364151b"
      },
      "source": [
        "! pip install PyAstronomy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyAstronomy\n",
            "  Downloading PyAstronomy-0.17.0.tar.gz (727 kB)\n",
            "\u001b[K     |████████████████████████████████| 727 kB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from PyAstronomy) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from PyAstronomy) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from PyAstronomy) (1.4.1)\n",
            "Collecting quantities\n",
            "  Downloading quantities-0.12.5.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting bidict\n",
            "  Downloading bidict-0.21.4-py3-none-any.whl (36 kB)\n",
            "Building wheels for collected packages: PyAstronomy, quantities\n",
            "  Building wheel for PyAstronomy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyAstronomy: filename=PyAstronomy-0.17.0-py3-none-any.whl size=522050 sha256=2b424c08a91207b77d28b2e1f7c4a714d881b3d142fcaa8a1fb5d5be019313ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/f4/cc/fe117c538c81443a6ba0e852ee8d69866a08e5163d2050aae5\n",
            "  Building wheel for quantities (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for quantities: filename=quantities-0.12.5-py3-none-any.whl size=80135 sha256=bac5c0fec37e8d3623600fa26077f810974e875348721ee5e3da05cbfbdc96a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/e7/32/0bb6d5bd0f619e583b6f1f4c710b535df898a1083e1e5d066c\n",
            "Successfully built PyAstronomy quantities\n",
            "Installing collected packages: quantities, bidict, PyAstronomy\n",
            "Successfully installed PyAstronomy-0.17.0 bidict-0.21.4 quantities-0.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9FmlGMqKdZ5",
        "outputId": "a6d1f9de-c13f-4770-addb-eac7588ca67d"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "from PyAstronomy import pyasl\n",
        "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import classification_report, roc_curve, auc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQe9R8dYKvGB"
      },
      "source": [
        "# read file (traffic.parquet contains all the records, can be found in Github)\n",
        "# please change data_path\n",
        "data_path = '/gdrive/MyDrive/traffic_stop/traffic.parquet'\n",
        "data = pd.read_parquet(data_path, engine = 'pyarrow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUqt4KKfLbDb"
      },
      "source": [
        "### Preprocessing: This is actually the step when I run out of RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48dMy4MUl4P0"
      },
      "source": [
        "keep_col = ['date', \n",
        "                'time', \n",
        "                'county_name', \n",
        "                'subject_race',\n",
        "                'subject_sex',\n",
        "                'violation', \n",
        "                'citation_issued',\n",
        "                'contraband_found',\n",
        "                'contraband_drugs',\n",
        "                'contraband_weapons',\n",
        "                'search_conducted',\n",
        "                'search_vehicle',\n",
        "                ]\n",
        "rm_na_col = ['county_name','violation','subject_race','subject_sex']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szCu087GlUw9"
      },
      "source": [
        "def remove_empty_rows(df, colName):\n",
        "\tdf = df[df[colName] != 'unknown']\n",
        "\tdf = df[df[colName].notna()]\n",
        "\treturn(df)\n",
        " \n",
        "def exclusive(vio_lst):\n",
        "  for vio in vio_lst:\n",
        "    # if there is any violation other than speeding, return 0\n",
        "    if 'speed' not in vio:\n",
        "      return 0\n",
        "  # if all of the violations (for this record) include speeding, return 1\n",
        "  return 1\n",
        "\n",
        "def preprocess(df, keep_col, rm_na_col, county_path, violation_type = None):\n",
        "\n",
        "  start = time.time()\n",
        "  # drop unrelated columns\n",
        "  col_drop = [col for col in df.columns if col not in keep_col]\n",
        "  df.drop(col_drop, axis = 1, inplace = True)\n",
        "\n",
        "  # remove rows with missing values in rm_na_col\n",
        "  for col in rm_na_col:\n",
        "    df = remove_empty_rows(df, col)\n",
        "\n",
        "  # convert violation to lower cases\n",
        "  df['violation'] = [s.lower() for s in df['violation']]\n",
        "\n",
        "  # if we only want to include certain violation type:\n",
        "  if violation_type == 'speed':\n",
        "    df = df.loc[df['violation'].str.contains('speed', regex=False),:]\n",
        "\n",
        "  # if we only want to include cases with no violations other than speeding\n",
        "  if violation_type == 'speed_exclusive':\n",
        "    df = df.loc[df['violation'].str.contains('speed', regex = False),:]\n",
        "    df['violation'] = df['violation'].map(lambda x: x.replace('(#)',''))\n",
        "    df['violation'] = df['violation'].map(lambda x: x.strip())\n",
        "    # get a list of violations for each record, and apply self-defined func exclusive\n",
        "    df['all_violation'] = df['violation'].str.split('|')\n",
        "    df['speeding_only'] = df['all_violation'].map(lambda x: exclusive(x))\n",
        "    # filter out rows with violations other than speeding\n",
        "    df = df.loc[df['speeding_only'] == 1,:]\n",
        "    df.drop(['speeding_only','all_violation'], axis = 1, inplace = True)\n",
        "\n",
        "  # after selecting based on violation, drop the column\n",
        "  df.drop('violation', 1, inplace=True)\n",
        "\n",
        "  # adding time of year info\n",
        "  df['year'] = pd.to_datetime(df['date']).dt.year\n",
        "  df['yearfrac'] = [pyasl.decimalYear(d) for d in pd.to_datetime(df['date'])]\n",
        "  df['yearfrac'] = df['yearfrac'] - df['year']\n",
        "  df['minute'] = df['time'].map(lambda x: x.minute)\n",
        "  df['hour'] = df['time'].map(lambda x: x.hour)\n",
        "  df['time'] = df['hour'] + df['minute'] / 60\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(np.array(df['time']).reshape(-1,1))\n",
        "  df['time'] = scaler.transform(np.array(df['time']).reshape(-1,1))\n",
        "  df.drop(['hour', 'minute', 'date', 'year'], 1, inplace=True)\n",
        "        \n",
        "  # County names are converted to county type - metropolitan, micropolitan or non-core\n",
        "  # For definitions, see US OMB website\n",
        "\n",
        "  # read in county info csv\n",
        "  # Please change the file path !!!!!\n",
        "  county_df = pd.read_csv(county_path)\n",
        "  county_df = county_df[county_df['State']=='Texas']\n",
        "  county_df = county_df.filter(items=['Metropolitan Status', 'County Name'])\n",
        "\n",
        "  # transform column\n",
        "  df['county_name'] = [name[:-7] for name in df['county_name']]\n",
        "  df = df.join(county_df.set_index('County Name'), on='county_name')\n",
        "  df.drop('county_name', 1, inplace=True)\n",
        "  df.rename(columns={'Metropolitan Status':'county_type'}, inplace=True)\n",
        "\n",
        "  # Convert citation issued and warning issued columns to integer\n",
        "  df = df.astype({'citation_issued': 'int64'})\n",
        "\n",
        "  # search and contraband related variables have three levels: None, True, False, if not True, use 0, else 1\n",
        "  for col in ['contraband_found','contraband_drugs','contraband_weapons','search_conducted','search_vehicle']:\n",
        "    df[col] = df[col].map({True: 1, False: 0, None: 0})\n",
        "\n",
        "  # if race is 'other'/'unknown', make them one level other/unknown\n",
        "  # df['subject_race'] = df['subject_race'].replace({'other':'other/unknown','unknown':'other/unknown'})\n",
        "  # update: if race is other/unknown, we delete the rows!\n",
        "  df = df.loc[(df['subject_race'] != 'unknown') & (df['subject_race'] != 'other'),:]\n",
        "  #print(df['subject_race'].value_counts())\n",
        "  # get dummies for race and sex\n",
        "  df = pd.get_dummies(df)\n",
        " \n",
        "  # Base level: White, Male, Non_core (county type)\n",
        "  df.drop(['subject_race_white', 'subject_sex_male', 'county_type_Non core','subject_race_other','subject_race_unknown'], axis = 1, inplace = True)\n",
        "  print(df.columns)\n",
        "  print('preprocessing time: %d'%(time.time()-start))\n",
        "  return df "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7jAWYdTTfBN"
      },
      "source": [
        "col_lst = ['time', 'citation_issued', 'contraband_found', 'contraband_drugs', \n",
        "           'contraband_weapons', 'search_conducted', 'search_vehicle', 'yearfrac',\n",
        "           'subject_race_asian/pacific islander', 'subject_race_black', 'subject_race_hispanic', 'subject_sex_female', \n",
        "           'county_type_Metropolitan', 'county_type_Micropolitan']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYbTjl99XwtA"
      },
      "source": [
        "### Run Analysis for 3 different kinds of violations (all, with speeding, only speeding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHxhY40KMXSL"
      },
      "source": [
        "num_feat = len(col_lst)\n",
        "# this df is later pass into run_year_analysis to get all estimates\n",
        "df_all = pd.DataFrame({'variable':col_lst})\n",
        "\n",
        "# keep track of coefficient estimates and evaluation metrics\n",
        "asianpacific = []\t\n",
        "black = []\t\n",
        "hispanic = []\t\n",
        "roc_auc = []\n",
        "\n",
        "vio_type = ['None','speed','speed_exclusive']\n",
        "def run_analysis(data, output_path, keep_col, rm_na_col, df_all, county_path, violation_type = vio_type):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    data_path: the folder path where all the yearly-based parquet files are saved\n",
        "\n",
        "    output_path: output path for csv files -> not used yet, because I want to print out the result and directly \n",
        "    save it afterwards. After we make sure there is no problem about how I did the analysis, we integrate this part\n",
        "    into the function\n",
        "\n",
        "    keep_col, rm_na_col,violation_type: parameters for preprocessing\n",
        "    df_years: an empty data frame where we can save the logistic regression results\n",
        "\n",
        "    Output:\n",
        "    A result dataframe\n",
        "    \"\"\"\n",
        "    for vio in vio_type:\n",
        "      df = preprocess(df = data,keep_col = keep_col, rm_na_col = rm_na_col, county_path = county_path, violation_type = vio)\n",
        "      # train test split\n",
        "      y = df['citation_issued']\n",
        "      X = df.drop('citation_issued', axis = 1)\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "      # note that in some columns, we could have all 0 values, this makes the matrix singular and cannot run logistic regression\n",
        "      # Those columns also contain no useful information. So, we drop those columns\n",
        "      for col in X.columns:\n",
        "        if len(X_train.loc[X_train[col] == 0,:]) == len(X_train):\n",
        "          X_train.drop(col, axis = 1, inplace = True)\n",
        "          X_test.drop(col, axis = 1, inplace = True)\n",
        "          print(vio, ': ','Delete ', col)\n",
        "\n",
        "      #adding constant to X\n",
        "      X_train_with_constant = sm.add_constant(X_train)\n",
        "      X_test_with_constant = sm.add_constant(X_test)\n",
        "      # building the model and fitting the data\n",
        "      log_reg = sm.Logit(y_train, X_train_with_constant).fit()\n",
        "          \n",
        "      res_df = pd.DataFrame({'variable':list(log_reg.params.index), 'coef':list(log_reg.params.values), 'odds_ratio':list(np.exp(log_reg.params.values)), 'pvalue':list(log_reg.pvalues)})\n",
        "      df_all = df_all.merge(res_df, left_on = 'variable', right_on = 'variable', how = 'left')\n",
        "      df_all = df_all.rename(columns = {'coef': ('coef_' + str(vio)), 'odds_ratio': ('odds_ratio_' + str(vio)), 'pvalue': ('pvalue_' + str(vio))})\n",
        "\n",
        "      # append race coefs to result list for plotting\n",
        "      black.append(log_reg.params['subject_race_black'])\n",
        "      hispanic.append(log_reg.params['subject_race_hispanic'])\n",
        "      asianpacific.append(log_reg.params['subject_race_asian/pacific islander'])\n",
        "\n",
        "      # for test set evaluation\n",
        "      y_pred = log_reg.predict(X_test_with_constant)\n",
        "      fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
        "      roc_auc.append(auc(fpr, tpr))\n",
        "      # for classification report, choose threshold 0.5\n",
        "      y_pred_c = [1 if y > 0.5 else 0 for y in y_pred]\n",
        "      print('Violation: ', vio )\n",
        "      print('-----------------------------------------------')\n",
        "      print(classification_report(y_test, y_pred_c))\n",
        "      print('-----------------------------------------------')\n",
        "\n",
        "    return df_all\n",
        "\n",
        "      # write result df to csv file\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSLG91a_Lh8L"
      },
      "source": [
        "# county csv file path\n",
        "county_path = '/gdrive/MyDrive/traffic_stop/2014-2018.csv'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcad2vUiVBcB"
      },
      "source": [
        "res_df = run_analysis(data, output_path = ' ', keep_col = keep_col, rm_na_col = rm_na_col, county_path = county_path, df_all = df_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "h-gTUmYFQZ6J",
        "outputId": "d9872b10-b23c-4816-9ffa-ad09747b2cb3"
      },
      "source": [
        "# res_df should look something like this (this table is obtained from smaller data, stats not valid) --- NaN means that variable not included in analysis because it only has a single value.\n",
        "res_df"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variable</th>\n",
              "      <th>coef_None</th>\n",
              "      <th>odds_ratio_None</th>\n",
              "      <th>pvalue_None</th>\n",
              "      <th>coef_speed</th>\n",
              "      <th>odds_ratio_speed</th>\n",
              "      <th>pvalue_speed</th>\n",
              "      <th>coef_speed_exclusive</th>\n",
              "      <th>odds_ratio_speed_exclusive</th>\n",
              "      <th>pvalue_speed_exclusive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>time</td>\n",
              "      <td>-0.265693</td>\n",
              "      <td>0.766674</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.089557</td>\n",
              "      <td>0.914336</td>\n",
              "      <td>5.635671e-22</td>\n",
              "      <td>-0.143230</td>\n",
              "      <td>0.866555</td>\n",
              "      <td>8.607800e-34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>citation_issued</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>contraband_found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>contraband_drugs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>contraband_weapons</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>search_conducted</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>search_vehicle</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>yearfrac</td>\n",
              "      <td>-0.118982</td>\n",
              "      <td>0.887824</td>\n",
              "      <td>8.864939e-103</td>\n",
              "      <td>-0.097669</td>\n",
              "      <td>0.906949</td>\n",
              "      <td>1.302140e-40</td>\n",
              "      <td>-0.106416</td>\n",
              "      <td>0.899051</td>\n",
              "      <td>1.347093e-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>subject_race_asian/pacific islander</td>\n",
              "      <td>0.229005</td>\n",
              "      <td>1.257349</td>\n",
              "      <td>1.124417e-73</td>\n",
              "      <td>0.532300</td>\n",
              "      <td>1.702844</td>\n",
              "      <td>2.198353e-265</td>\n",
              "      <td>0.635356</td>\n",
              "      <td>1.887694</td>\n",
              "      <td>2.220594e-288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>subject_race_black</td>\n",
              "      <td>0.485210</td>\n",
              "      <td>1.624517</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.508686</td>\n",
              "      <td>1.663105</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.259997</td>\n",
              "      <td>1.296927</td>\n",
              "      <td>3.510297e-177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>subject_race_hispanic</td>\n",
              "      <td>0.451628</td>\n",
              "      <td>1.570867</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.540860</td>\n",
              "      <td>1.717483</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.260513</td>\n",
              "      <td>1.297596</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>subject_sex_female</td>\n",
              "      <td>-0.131235</td>\n",
              "      <td>0.877011</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.143024</td>\n",
              "      <td>0.866733</td>\n",
              "      <td>4.326495e-229</td>\n",
              "      <td>-0.062363</td>\n",
              "      <td>0.939542</td>\n",
              "      <td>2.232961e-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>county_type_Metropolitan</td>\n",
              "      <td>0.391151</td>\n",
              "      <td>1.478682</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.379882</td>\n",
              "      <td>1.462112</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.304975</td>\n",
              "      <td>1.356591</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>county_type_Micropolitan</td>\n",
              "      <td>-0.161934</td>\n",
              "      <td>0.850498</td>\n",
              "      <td>1.472632e-216</td>\n",
              "      <td>-0.197173</td>\n",
              "      <td>0.821049</td>\n",
              "      <td>1.671220e-197</td>\n",
              "      <td>-0.282332</td>\n",
              "      <td>0.754023</td>\n",
              "      <td>4.438298e-261</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               variable  ...  pvalue_speed_exclusive\n",
              "0                                  time  ...            8.607800e-34\n",
              "1                       citation_issued  ...                     NaN\n",
              "2                      contraband_found  ...                     NaN\n",
              "3                      contraband_drugs  ...                     NaN\n",
              "4                    contraband_weapons  ...                     NaN\n",
              "5                      search_conducted  ...                     NaN\n",
              "6                        search_vehicle  ...                     NaN\n",
              "7                              yearfrac  ...            1.347093e-31\n",
              "8   subject_race_asian/pacific islander  ...           2.220594e-288\n",
              "9                    subject_race_black  ...           3.510297e-177\n",
              "10                subject_race_hispanic  ...            0.000000e+00\n",
              "11                   subject_sex_female  ...            2.232961e-30\n",
              "12             county_type_Metropolitan  ...            0.000000e+00\n",
              "13             county_type_Micropolitan  ...           4.438298e-261\n",
              "\n",
              "[14 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stux41x3VhxG"
      },
      "source": [
        "# print odds ratios\n",
        "asianpacific = np.exp(asianpacific)\n",
        "black = np.exp(black)\n",
        "hispanic = np.exp(hispanic)\n",
        "print(asianpacific)\n",
        "print(black)\n",
        "print(hispanic)\n",
        "print(roc_auc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}