{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains code for calculating missingness."
      ],
      "metadata": {
        "id": "FsHTMCdXO-X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Import Libraries and Mount Drive*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NkWFKrHFPOqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h9K3lsaaO4tm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q_3fvdqPGfK",
        "outputId": "a9a5072b-10ed-4ea5-9eb8-5743eae8f091"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def size(year):\n",
        "  df = pd.read_parquet('/content/drive/MyDrive/traffic_stop/year_data/traffic_' + str(year) + '.parquet')\n",
        "  return (len(df.index))"
      ],
      "metadata": {
        "id": "Gflx_nyIEQuf"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year06= size(2006)\n",
        "year06 = 0 if year06 is None else int(year06)\n",
        "year07= size(2007)\n",
        "year07 = 0 if year07 is None else int(year07)\n",
        "year08= size(2008)\n",
        "year08 = 0 if year08 is None else int(year08)\n",
        "year09= size(2009)\n",
        "year09 = 0 if year09 is None else int(year09)\n",
        "year10= size(2010)\n",
        "year10 = 0 if year10 is None else int(year10)\n",
        "year11= size(2011)\n",
        "year11 = 0 if year11 is None else int(year11)\n",
        "year12= size(2012)\n",
        "year12 = 0 if year12 is None else int(year12)\n",
        "year13= size(2013)\n",
        "year13 = 0 if year13 is None else int(year13)\n",
        "year14= size(2014)\n",
        "year14 = 0 if year14 is None else int(year14)\n",
        "year15= size(2015)\n",
        "year15 = 0 if year15 is None else int(year15)\n",
        "year16= size(2016)\n",
        "year16 = 0 if year16 is None else int(year16)\n",
        "year17= size(2017)\n",
        "year17 = 0 if year17 is None else int(year17)"
      ],
      "metadata": {
        "id": "N8HZbDEQIUwh"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_vio_types(threshold, year_start, year_end):\n",
        "  \"\"\"\n",
        "  data format: violation type in data end with ' (#)'; more than 1 violation types: 'A|B'\n",
        "  returns a list of violation types with # of single violation records > threshold every year\n",
        "  \"\"\"\n",
        "  year_present_vios = {}\n",
        "  years = list(range(year_start, year_end + 1))\n",
        "\n",
        "  # get number of years in which a specific violation having # of single violation records more than threshold\n",
        "\n",
        "  for year in years:\n",
        "    print('Processing: ',year)\n",
        "    df = pd.read_parquet('/content/drive/MyDrive/traffic_stop/year_data/traffic_' + str(year) + '.parquet')\n",
        "\n",
        "    cnt_df = pd.DataFrame(df['violation'].value_counts(dropna=False))\n",
        "    \n",
        "    cnt_df = cnt_df.loc[cnt_df['violation'] > threshold]\n",
        "    single_vio_lst = [vio for vio in list(cnt_df.index) if '|' not in vio]\n",
        "\n",
        "    for vio in single_vio_lst:\n",
        "      if vio not in year_present_vios:\n",
        "        year_present_vios[vio] = 1\n",
        "      else:\n",
        "        year_present_vios[vio] += 1\n",
        "\n",
        "  vio_type_all = []\n",
        "  for vio, cnt in year_present_vios.items():\n",
        "    if cnt == len(years):\n",
        "      vio_type_all.append(vio)\n",
        "  vio_type_all = [vio.replace(' (#)', '').strip() for vio in vio_type_all]\n",
        "\n",
        "  return vio_type_all"
      ],
      "metadata": {
        "id": "M9qf6ItcP5We"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vio_vars = single_vio_types(threshold = 5000, year_start = 2006, year_end = 2017)"
      ],
      "metadata": {
        "id": "tkNthBVfQo9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function is called by preprocess(year, keep_col, rm_na_col, violation_types)\n",
        "def get_vio_df(violation_types, vio_col):\n",
        "  vio_df_dict  = {vio: [] for vio in violation_types}\n",
        "  vio_df_dict['others'] = []\n",
        "\n",
        "  for vio in vio_col:\n",
        "    vio_lst = vio.split('|')\n",
        "    vio_lst = [vio.replace(' (#)', '').strip() for vio in vio_lst]\n",
        "    # if single violation\n",
        "    if len(vio_lst) == 1:\n",
        "      vio = vio.replace(' (#)', '').strip()\n",
        "      # if vio in violation_types\n",
        "      if vio in violation_types:\n",
        "        vio_df_dict[vio].append(1)\n",
        "        vio_df_dict['others'].append(0)\n",
        "        for vio_var in violation_types:\n",
        "          if vio_var != vio:\n",
        "            vio_df_dict[vio_var].append(0)\n",
        "\n",
        "      # if vio not in violation_types\n",
        "      else:\n",
        "        vio_df_dict['others'].append(1)\n",
        "        for vio_var in violation_types:\n",
        "          vio_df_dict[vio_var].append(0)\n",
        "    \n",
        "    # if multiple violations\n",
        "    if len(vio_lst) > 1:\n",
        "\n",
        "      # for single vio variables\n",
        "      for vio_var in violation_types:\n",
        "        # if has one of violation_types\n",
        "        if vio_var in vio_lst:\n",
        "          vio_df_dict[vio_var].append(1)\n",
        "        # if violation_type not present\n",
        "        else:\n",
        "          vio_df_dict[vio_var].append(0)\n",
        "\n",
        "      # for the 'others' variable\n",
        "      # if have one vio not in violation_types, 'other' = 1\n",
        "      vars_vio_cnt = 0\n",
        "      for violation in vio_lst:\n",
        "        if violation not in violation_types:\n",
        "          vio_df_dict['others'].append(1)\n",
        "          break\n",
        "        else:\n",
        "          vars_vio_cnt += 1\n",
        "      # if all violations of this record in violation_types\n",
        "      if vars_vio_cnt == len(vio_lst):\n",
        "        vio_df_dict['others'].append(0)\n",
        "\n",
        "  vio_df = pd.DataFrame(vio_df_dict)\n",
        "  vio_df['violation'] = vio_col.values\n",
        "\n",
        "  return vio_df"
      ],
      "metadata": {
        "id": "D4Buk6vjQrC6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(row):\n",
        "  if row['speeding_only'] == 'speeding-repeated_entries':\n",
        "    return row['all_violation'][0].strip()\n",
        "  else:\n",
        "    return row['violation']\n",
        "\n",
        "def remove_white_spaces(vio_lst):\n",
        "  vio_lst = [x.strip() for x in vio_lst]\n",
        "  return vio_lst\n",
        "  \n",
        "def exclusive(vio_lst):\n",
        "  count = 0\n",
        "  for vio in vio_lst:\n",
        "    # count number of violations containing 'speed'\n",
        "    if 'speed' in vio:\n",
        "      count += 1\n",
        "  # if we have non-speeding violation\n",
        "  if count < len(vio_lst):\n",
        "    return 'speeding + others'\n",
        "  # if we only have speeding violation\n",
        "  elif count == len(vio_lst):\n",
        "    # if one speeding vio type\n",
        "    if count == 1:\n",
        "      return 'speeding-1'\n",
        "    # if we have more than one entries but only one type\n",
        "    elif len(set(vio_lst)) == 1:\n",
        "      return 'speeding-repeated_entries'\n",
        "    # if we have more than one type\n",
        "    else:\n",
        "      return 'speeding-multiple'\n",
        "  else:\n",
        "    return 'undefined case'"
      ],
      "metadata": {
        "id": "K3jsTYAMQus4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def speeding_filter(df):\n",
        "  print('Before speed filtering: ', len(df))\n",
        "  df['violation'] = [s.lower() for s in df['violation']]\n",
        "  # at least one violation is speeding-violated (could have other violations at the same time)\n",
        "  df = df.loc[df['violation'].str.contains('speed', regex = False),:]\n",
        "  df['violation'] = df['violation'].map(lambda x: x.replace('(#)',''))\n",
        "  df['violation'] = df['violation'].map(lambda x: x.strip())\n",
        "\n",
        "  # get a list of violations for each record, and apply self-defined func exclusive\n",
        "  df['all_violation'] = df['violation'].str.split('|')\n",
        "  df['all_violation'] = df['all_violation'].map(lambda x: remove_white_spaces(x))\n",
        "  df['speeding_only'] = df['all_violation'].map(lambda x: exclusive(x))\n",
        "\n",
        "  # filter out rows with violations other than speeding\n",
        "  df = df.loc[df['speeding_only'].isin(['speeding-repeated_entries','speeding-1','speeding-multiple']),:]\n",
        "\n",
        "  df['violation'] = df.apply(lambda row: remove_duplicates(row), axis = 1)\n",
        "\n",
        "  df.drop(['all_violation','speeding_only'], axis = 1, inplace = True)\n",
        "  print('Speeding only violation has records: ', len(df))\n",
        "  return df"
      ],
      "metadata": {
        "id": "8VdFKUEbQ79U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns we need in the analysis\n",
        "keep_col = [\n",
        "            'county_name', \n",
        "            'subject_race',\n",
        "            'subject_sex',\n",
        "            'violation', \n",
        "            'citation_issued'\n",
        "            ]\n",
        "\n",
        "# delete rows with missing value in these columns           \n",
        "rm_na_col = ['county_name','violation','subject_race','subject_sex']\n",
        "\n",
        "# this function is called by preprocess\n",
        "def remove_empty_rows(df, colName):\n",
        "\tdf = df[df[colName] != 'unknown']\n",
        "\tdf = df[df[colName].notna()]\n",
        "\treturn(df)"
      ],
      "metadata": {
        "id": "etg0W8guQ97A"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### this function used in run_year_analysis()\n",
        "def data_loss(year, keep_col, rm_na_col, with_vio = False, violation_types=None, speeding_only = False):\n",
        "  \"\"\"\n",
        "  with_vio: if include vioation type variables: True, if not include, False\n",
        "  violation_types: violation types to be included in the analysis\n",
        "  \"\"\"\n",
        "  # read dataframe\n",
        "  filename = '/content/drive/MyDrive/traffic_stop/year_data/traffic_' + str(year) + '.parquet'\n",
        "  df = pd.read_parquet(filename, engine = 'pyarrow')\n",
        "  print('# of all traffic stops in year ', year, ': ', len(df))\n",
        "  before = len(df)\n",
        "\n",
        "  # invalid value (IF NOT TEXAS STATE DATA, DELETE THIS INVALID VALUE BLOCK)\n",
        "  if year == 2013:\n",
        "    df.drop(df.index[df['lat'] == 74.052879], inplace=True)\n",
        "\n",
        "  # drop unrelated columns\n",
        "  col_drop = [col for col in df.columns if col not in keep_col]\n",
        "  df.drop(col_drop, axis = 1, inplace = True)\n",
        "\n",
        "  # remove rows with missing values in rm_na_col\n",
        "  for col in rm_na_col:\n",
        "    df = remove_empty_rows(df, col)\n",
        "\n",
        "  # filter rows if speeding_only\n",
        "  if speeding_only:\n",
        "    # filter: only speeding violation (no other violations) \n",
        "    df = speeding_filter(df)\n",
        "\n",
        "  ######### THIS WHOLE BLOACK NEEDS TO BE CHANGED, NEW METROPOLITAN FILES AND DIFFERENT DATA FORMAT #########\n",
        "\n",
        "  # County names are converted to county type - metropolitan, micropolitan or non-core\n",
        "  # For definitions, see US OMB website\n",
        "\n",
        "  # read in county info csv\n",
        "  county_df = pd.read_csv('/content/drive/MyDrive/traffic_stop/2014-2018.csv')\n",
        "  county_df = county_df[county_df['State']=='Texas']\n",
        "  county_df = county_df.filter(items=['Metropolitan Status', 'County Name'])\n",
        "\n",
        "  # transform column\n",
        "  df['county'] = [name[:-7] for name in df['county_name']]\n",
        "  df['county'] = df['county'].replace('Dewitt','DeWitt')\n",
        "  df = df.join(county_df.set_index('County Name'), on='county')\n",
        "  df.drop('county', axis = 1, inplace=True)\n",
        "  df.rename(columns={'Metropolitan Status':'county_type'}, inplace=True)\n",
        "\n",
        "  ######### Metropolitan block ends here #########\n",
        "\n",
        "  # Convert 'citation issued' to integer\n",
        "  df = df.astype({'citation_issued': 'int64'})\n",
        "\n",
        "  # if race is other/unknown, we delete the rows!\n",
        "  df = df.loc[(df['subject_race'] != 'unknown') & (df['subject_race'] != 'other'),:]\n",
        "  df['subject_race'] = df.subject_race.cat.remove_unused_categories()\n",
        "\n",
        "  df.drop(['county_name'], axis = 1, inplace = True)\n",
        "\n",
        "  if with_vio:\n",
        "\n",
        "    # get violation variables\n",
        "    vio_df = get_vio_df(violation_types, vio_col = df['violation'])\n",
        "    vio_df.drop('violation', axis = 1, inplace = True)\n",
        "\n",
        "    # other x variables get dummies\n",
        "    y = df['citation_issued']\n",
        "    df.drop(['violation','citation_issued'], axis = 1, inplace = True)\n",
        "    df = pd.get_dummies(df)\n",
        "    df.drop(['subject_race_white', 'county_type_Metropolitan', 'subject_sex_male'], axis = 1, inplace = True)\n",
        "\n",
        "    #vio_df.drop('Speeding Over Limit', axis = 1, inplace = True), I don't think vio_type needs a base level, as no multicollinearity\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    vio_df.reset_index(drop=True, inplace=True)\n",
        "    y.reset_index(drop=True, inplace=True)\n",
        "    X = pd.concat([df, vio_df], axis = 1)\n",
        "    X.columns = list(df.columns) + list(vio_df.columns)\n",
        "\n",
        "    print('# of traffic stops after preprocessing in year ', year, ': ', len(X))\n",
        "    after = len(X)\n",
        "    print('Data that is lost in year', year, ':')\n",
        "    return (before-after)\n",
        "\n",
        "  elif not with_vio:\n",
        "\n",
        "    # other x variables get dummies\n",
        "    y = df['citation_issued']\n",
        "    df.drop(['violation','citation_issued'], axis = 1, inplace = True)\n",
        "    df = pd.get_dummies(df)\n",
        "    df.drop(['subject_race_white', 'county_type_Metropolitan', 'subject_sex_male'], axis = 1, inplace = True)\n",
        "    \n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    print('# of traffic stops after preprocessing in year ', year, ': ', len(df))\n",
        "    after = len(df)\n",
        "\n",
        "    print('Data that is lost in year', year, ':')\n",
        "    return (before-after)\n",
        "  "
      ],
      "metadata": {
        "id": "FlpCx8pFA7QI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for year in range(2006, 2018):\n",
        "  data_lost=(data_loss(year, keep_col, rm_na_col, with_vio = False, violation_types=None, speeding_only = False))\n",
        "  print(data_lost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gLtPyzCExUI",
        "outputId": "89b924ab-7611-4b2f-9d8e-6924757164b0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of all traffic stops in year  2006 :  2693894\n",
            "# of traffic stops after preprocessing in year  2006 :  2671708\n",
            "Data that is lost in year 2006 :\n",
            "22186\n",
            "# of all traffic stops in year  2007 :  2427347\n",
            "# of traffic stops after preprocessing in year  2007 :  2404322\n",
            "Data that is lost in year 2007 :\n",
            "23025\n",
            "# of all traffic stops in year  2008 :  2526783\n",
            "# of traffic stops after preprocessing in year  2008 :  2432074\n",
            "Data that is lost in year 2008 :\n",
            "94709\n",
            "# of all traffic stops in year  2009 :  2441306\n",
            "# of traffic stops after preprocessing in year  2009 :  2368223\n",
            "Data that is lost in year 2009 :\n",
            "73083\n",
            "# of all traffic stops in year  2010 :  2525296\n",
            "# of traffic stops after preprocessing in year  2010 :  2449377\n",
            "Data that is lost in year 2010 :\n",
            "75919\n",
            "# of all traffic stops in year  2011 :  2588004\n",
            "# of traffic stops after preprocessing in year  2011 :  2508218\n",
            "Data that is lost in year 2011 :\n",
            "79786\n",
            "# of all traffic stops in year  2012 :  2435812\n",
            "# of traffic stops after preprocessing in year  2012 :  2364658\n",
            "Data that is lost in year 2012 :\n",
            "71154\n",
            "# of all traffic stops in year  2013 :  2134936\n",
            "# of traffic stops after preprocessing in year  2013 :  2071465\n",
            "Data that is lost in year 2013 :\n",
            "63471\n",
            "# of all traffic stops in year  2014 :  1878488\n",
            "# of traffic stops after preprocessing in year  2014 :  1816151\n",
            "Data that is lost in year 2014 :\n",
            "62337\n",
            "# of all traffic stops in year  2015 :  1745385\n",
            "# of traffic stops after preprocessing in year  2015 :  1678602\n",
            "Data that is lost in year 2015 :\n",
            "66783\n",
            "# of all traffic stops in year  2016 :  1832207\n",
            "# of traffic stops after preprocessing in year  2016 :  1830586\n",
            "Data that is lost in year 2016 :\n",
            "1621\n",
            "# of all traffic stops in year  2017 :  2197382\n",
            "# of traffic stops after preprocessing in year  2017 :  2195417\n",
            "Data that is lost in year 2017 :\n",
            "1965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_data= year06+year07+year08+year09+year10+year11+year12+year13+year14+year15+year16+year17"
      ],
      "metadata": {
        "id": "9VOT1DXoHXMU"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data_lost=22186+23025+94709+73083+75919+79786+71154+63471+62337+66783+1621+1965\n",
        "print('Total Data Lost across all years: ', total_data_lost)\n",
        "\n",
        "print('Total Data before any processing across all years: ', total_data)\n",
        "\n",
        "percentage_lost=(total_data_lost/total_data)*100\n",
        "print('Percentage of lost data across all years is: ', percentage_lost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOkfMVPsGuXH",
        "outputId": "6c633ef4-7134-4ba5-bd02-1d0f189e9a61"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Data Lost across all years:  636039\n",
            "Total Data before any processing across all years:  27426840\n",
            "Percentage of lost data across all years is:  2.319038576810161\n"
          ]
        }
      ]
    }
  ]
}